# 深度学习/计算机视觉基础知识

梯度消失
-------

**产生原因**：对于传统的激活函数如sigmoid，tanh，在输入的较大与较小区域存在梯度饱和区，当输入进入饱和区，梯度趋于0；输入经过多层网络后，在不经过处理的情况下，会导致分布趋于激活函数的饱和区，因此反向传播时，在激活函数处梯度趋于0，产生梯度消失，距离输出越远的层参数受到的训练越微小；
**解决方法**：
    1. 激活函数角度，使用ReLU系激活函数，避免一定程度饱和区的影响；
    2. 数据分布角度，使用BN层等normalization方法，将激活函数的输入进行分布的归一化处理，使其位于激活函数的非饱和区；
    3. 参数初始化角度，使用Xavier，Msra初始化以保证数据在参数处理前后的方差保持一致；
    4. 使用ResNet的残差模块或denseNet等，通过短接分支，保证了信息的完整性，简化问题为学习输入与目标的差，缓解ReLU的神经元坏死问题；
